#!/usr/bin/env python3
"""
Create a LangGraph agent with Genie integration.

This script generates a complete agent implementation that can query
Databricks Genie spaces using LangGraph.
"""

import argparse
import os
from pathlib import Path
from typing import Optional


AGENT_TEMPLATE = '''"""
{agent_name} - LangGraph agent with Databricks Genie integration.

Generated by create_genie_agent.py
"""

import os
from typing import TypedDict, Annotated, Sequence, Optional
import operator

from langchain_core.messages import BaseMessage, HumanMessage, AIMessage
from langgraph.graph import StateGraph, END
from databricks_langchain import ChatDatabricks
import mlflow

from genie_client import GenieClient, format_as_markdown_table


# Agent State Schema
class {state_class}(TypedDict):
    """State for {agent_name}."""
    messages: Annotated[Sequence[BaseMessage], operator.add]
    genie_space_id: str
    conversation_id: Optional[str]
    query_results: list
    next_action: str


# Initialize Genie client
genie_client = GenieClient(
    host=os.getenv("DATABRICKS_HOST"),
    token=os.getenv("DATABRICKS_TOKEN")
)

# Initialize LLM
llm = ChatDatabricks(
    endpoint=os.getenv("DATABRICKS_LLM_ENDPOINT", "databricks-meta-llama-3-1-70b-instruct"),
    temperature=0.1
)


def analyze_query_node(state: {state_class}) -> {state_class}:
    """Analyze if query requires Genie."""
    user_message = state["messages"][-1].content

    analysis_prompt = f"""
    Analyze this user query: {{user_message}}

    Does this require querying a data warehouse or database?
    Answer with only YES or NO.
    """

    response = llm.invoke(analysis_prompt)
    needs_data = "YES" in response.content.upper()

    return {{
        "next_action": "query_genie" if needs_data else "respond_directly"
    }}


def query_genie_node(state: {state_class}) -> {state_class}:
    """Query Genie space for data."""
    user_query = state["messages"][-1].content
    space_id = state["genie_space_id"]
    conversation_id = state.get("conversation_id")

    try:
        # Query Genie
        if conversation_id:
            message = genie_client.continue_conversation(
                space_id=space_id,
                conversation_id=conversation_id,
                content=user_query
            )
        else:
            message = genie_client.start_conversation(
                space_id=space_id,
                content=user_query
            )
            conversation_id = message.id.split("/")[0]  # Extract conversation ID

        # Extract data
        data = genie_client.extract_data(message)

        return {{
            "conversation_id": conversation_id,
            "query_results": state.get("query_results", []) + [data],
            "next_action": "format_response"
        }}

    except Exception as e:
        # Handle errors
        error_msg = f"Error querying Genie: {{str(e)}}"
        return {{
            "messages": [AIMessage(content=error_msg)],
            "next_action": "end"
        }}


def format_response_node(state: {state_class}) -> {state_class}:
    """Format Genie results into natural language response."""
    user_query = state["messages"][-1].content
    latest_result = state["query_results"][-1]

    # Format as markdown table
    table = format_as_markdown_table(latest_result)

    # Generate natural language summary
    format_prompt = f"""
    User asked: {{user_query}}

    Query results:
    {{table}}

    Provide a concise, natural language summary of the key insights from this data.
    Include the most important numbers and trends.
    """

    response = llm.invoke(format_prompt)

    # Combine table and summary
    full_response = f"{{response.content}}\\n\\n**Detailed Results:**\\n{{table}}"

    return {{
        "messages": [AIMessage(content=full_response)],
        "next_action": "end"
    }}


def respond_directly_node(state: {state_class}) -> {state_class}:
    """Respond without querying Genie."""
    user_message = state["messages"][-1].content

    response = llm.invoke(user_message)

    return {{
        "messages": [AIMessage(content=response.content)],
        "next_action": "end"
    }}


# Build the agent graph
def create_agent(space_id: str) -> StateGraph:
    """Create compiled agent graph."""

    # Create graph
    graph = StateGraph({state_class})

    # Add nodes
    graph.add_node("analyze_query", analyze_query_node)
    graph.add_node("query_genie", query_genie_node)
    graph.add_node("format_response", format_response_node)
    graph.add_node("respond_directly", respond_directly_node)

    # Define routing
    def route_after_analysis(state: {state_class}) -> str:
        return state["next_action"]

    # Add edges
    graph.set_entry_point("analyze_query")
    graph.add_conditional_edges(
        "analyze_query",
        route_after_analysis,
        {{
            "query_genie": "query_genie",
            "respond_directly": "respond_directly"
        }}
    )
    graph.add_edge("query_genie", "format_response")
    graph.add_edge("format_response", END)
    graph.add_edge("respond_directly", END)

    # Compile
    return graph.compile()


# Example usage
if __name__ == "__main__":
    import sys

    if len(sys.argv) < 3:
        print("Usage: python {agent_name}.py <space_id> <query>")
        sys.exit(1)

    space_id = sys.argv[1]
    query = sys.argv[2]

    # Enable MLflow tracing
    mlflow.langchain.autolog()

    # Create agent
    agent = create_agent(space_id)

    # Run agent
    with mlflow.start_run():
        result = agent.invoke({{
            "messages": [HumanMessage(content=query)],
            "genie_space_id": space_id,
            "conversation_id": None,
            "query_results": [],
            "next_action": ""
        }})

        # Print response
        print("\\nAgent Response:")
        print(result["messages"][-1].content)

        # Log to MLflow
        mlflow.log_param("genie_space_id", space_id)
        mlflow.log_param("query", query)
        mlflow.log_metric("message_count", len(result["messages"]))
'''


MULTI_AGENT_TEMPLATE = '''"""
{agent_name} - Multi-agent system with Genie supervisor.

Generated by create_genie_agent.py
"""

import os
from typing import TypedDict, Annotated, Sequence, Literal
import operator

from langchain_core.messages import BaseMessage, HumanMessage, AIMessage
from langgraph.graph import StateGraph, END
from databricks_langchain import ChatDatabricks
import mlflow

from genie_client import GenieClient, format_as_markdown_table


# Configuration
GENIE_SPACES = {{
    {space_config}
}}


# Agent State
class SupervisorState(TypedDict):
    """State for supervisor agent."""
    messages: Annotated[Sequence[BaseMessage], operator.add]
    selected_agent: str
    final_response: str


# Initialize clients
genie_client = GenieClient()
llm = ChatDatabricks(
    endpoint=os.getenv("DATABRICKS_LLM_ENDPOINT", "databricks-meta-llama-3-1-70b-instruct")
)


def supervisor_node(state: SupervisorState) -> SupervisorState:
    """Route query to appropriate Genie agent."""
    user_query = state["messages"][-1].content

    # Format agent descriptions
    agent_list = "\\n".join([
        f"- {{name}}: {{config['description']}}"
        for name, config in GENIE_SPACES.items()
    ])

    routing_prompt = f"""
    User query: {{user_query}}

    Available specialized agents:
    {{agent_list}}

    Which agent should handle this query? Respond with the agent name only.
    If no agent is appropriate, respond with "general".
    """

    response = llm.invoke(routing_prompt)
    selected = response.content.strip().lower()

    # Validate selection
    if selected not in GENIE_SPACES and selected != "general":
        selected = "general"

    return {{"selected_agent": selected}}


def create_genie_agent_node(agent_name: str, space_id: str):
    """Factory function to create Genie agent nodes."""

    def agent_node(state: SupervisorState) -> SupervisorState:
        user_query = state["messages"][-1].content

        try:
            # Query Genie
            message = genie_client.start_conversation(space_id, user_query)
            data = genie_client.extract_data(message)

            # Format results
            table = format_as_markdown_table(data)

            # Generate summary
            summary_prompt = f"""
            User asked: {{user_query}}

            Results from {{agent_name}}:
            {{table}}

            Provide a natural language summary of the key insights.
            """

            summary = llm.invoke(summary_prompt)
            full_response = f"{{summary.content}}\\n\\n**Data:**\\n{{table}}"

            return {{
                "messages": [AIMessage(content=full_response)],
                "final_response": full_response
            }}

        except Exception as e:
            error_msg = f"Error from {{agent_name}}: {{str(e)}}"
            return {{
                "messages": [AIMessage(content=error_msg)],
                "final_response": error_msg
            }}

    return agent_node


def general_agent_node(state: SupervisorState) -> SupervisorState:
    """Handle general queries without Genie."""
    user_query = state["messages"][-1].content
    response = llm.invoke(user_query)

    return {{
        "messages": [AIMessage(content=response.content)],
        "final_response": response.content
    }}


def create_multi_agent() -> StateGraph:
    """Create multi-agent system with supervisor."""

    # Create graph
    graph = StateGraph(SupervisorState)

    # Add supervisor
    graph.add_node("supervisor", supervisor_node)

    # Add specialized Genie agents
    for agent_name, config in GENIE_SPACES.items():
        node_func = create_genie_agent_node(agent_name, config["space_id"])
        graph.add_node(agent_name, node_func)
        graph.add_edge(agent_name, END)

    # Add general agent
    graph.add_node("general", general_agent_node)
    graph.add_edge("general", END)

    # Routing function
    def route_to_agent(state: SupervisorState) -> str:
        return state["selected_agent"]

    # Add conditional edges
    routes = {{name: name for name in GENIE_SPACES.keys()}}
    routes["general"] = "general"

    graph.add_conditional_edges("supervisor", route_to_agent, routes)
    graph.set_entry_point("supervisor")

    return graph.compile()


if __name__ == "__main__":
    import sys

    if len(sys.argv) < 2:
        print("Usage: python {agent_name}.py <query>")
        sys.exit(1)

    query = sys.argv[1]

    # Enable tracing
    mlflow.langchain.autolog()

    # Create multi-agent
    agent = create_multi_agent()

    # Run
    with mlflow.start_run():
        result = agent.invoke({{
            "messages": [HumanMessage(content=query)],
            "selected_agent": "",
            "final_response": ""
        }})

        print("\\nAgent Response:")
        print(result["final_response"])
'''


def generate_agent(
    agent_name: str,
    agent_type: str,
    output_dir: Path,
    space_id: Optional[str] = None,
    spaces_config: Optional[dict] = None
):
    """Generate agent implementation file."""

    # Prepare template variables
    state_class = f"{agent_name.replace('_', ' ').title().replace(' ', '')}State"

    if agent_type == "single":
        if not space_id:
            raise ValueError("space_id required for single agent")

        content = AGENT_TEMPLATE.format(
            agent_name=agent_name,
            state_class=state_class
        )
    elif agent_type == "multi":
        if not spaces_config:
            raise ValueError("spaces_config required for multi-agent")

        # Format space configuration
        space_lines = []
        for name, config in spaces_config.items():
            space_lines.append(
                f'    "{name}": {{"space_id": "{config["space_id"]}", "description": "{config["description"]}"}}'
            )
        space_config = ",\n".join(space_lines)

        content = MULTI_AGENT_TEMPLATE.format(
            agent_name=agent_name,
            space_config=space_config
        )
    else:
        raise ValueError(f"Unknown agent type: {agent_type}")

    # Write file
    output_file = output_dir / f"{agent_name}.py"
    output_file.write_text(content)

    print(f"✓ Generated: {output_file}")

    return output_file


def main():
    """Main entry point."""
    parser = argparse.ArgumentParser(
        description="Generate LangGraph agent with Genie integration"
    )
    parser.add_argument("agent_name", help="Name of the agent")
    parser.add_argument(
        "--type",
        choices=["single", "multi"],
        default="single",
        help="Agent type: single Genie space or multi-agent supervisor"
    )
    parser.add_argument(
        "--space-id",
        help="Genie space ID (for single agent)"
    )
    parser.add_argument(
        "--spaces",
        help="JSON file with spaces config (for multi-agent)"
    )
    parser.add_argument(
        "-o", "--output",
        default=".",
        help="Output directory"
    )

    args = parser.parse_args()

    output_dir = Path(args.output)
    output_dir.mkdir(parents=True, exist_ok=True)

    # Validate inputs
    if args.type == "single" and not args.space_id:
        parser.error("--space-id required for single agent")

    if args.type == "multi" and not args.spaces:
        parser.error("--spaces required for multi-agent")

    # Load spaces config for multi-agent
    spaces_config = None
    if args.type == "multi":
        import json
        with open(args.spaces) as f:
            spaces_config = json.load(f)

    # Generate agent
    agent_file = generate_agent(
        agent_name=args.agent_name,
        agent_type=args.type,
        output_dir=output_dir,
        space_id=args.space_id,
        spaces_config=spaces_config
    )

    print(f"\\n✓ Agent generated successfully!")
    print(f"\\nNext steps:")
    print(f"1. Review and customize: {agent_file}")
    print(f"2. Set environment variables:")
    print(f"   export DATABRICKS_HOST=https://your-workspace.cloud.databricks.com")
    print(f"   export DATABRICKS_TOKEN=dapi...")
    print(f"3. Run the agent:")
    print(f"   python {agent_file} <space_id> <query>")


if __name__ == "__main__":
    main()
