#!/usr/bin/env python3
"""
Create a LangGraph RAG agent with Databricks Vector Search.

This script generates complete RAG agent implementations that use
Databricks Vector Search for document retrieval.
"""

import argparse
import os
from pathlib import Path
from typing import Optional, List


SIMPLE_RAG_TEMPLATE = '''"""
{agent_name} - Simple RAG agent with Databricks Vector Search.

Generated by create_rag_agent.py
"""

import os
from typing import TypedDict, Annotated, Sequence
import operator

from langchain_core.messages import BaseMessage, HumanMessage, AIMessage
from langgraph.graph import StateGraph, END
from databricks_langchain import ChatDatabricks
import mlflow

from vector_search_retriever import create_retriever


# Agent State
class {state_class}(TypedDict):
    """State for {agent_name}."""
    messages: Annotated[Sequence[BaseMessage], operator.add]
    retrieved_documents: list
    final_answer: str


# Initialize components
llm = ChatDatabricks(
    endpoint=os.getenv("DATABRICKS_LLM_ENDPOINT", "databricks-meta-llama-3-1-70b-instruct"),
    temperature=0.1
)

retriever = create_retriever(
    index_name="{index_name}",
    endpoint_name="{endpoint_name}",
    num_results={num_results}
)


def retrieve_node(state: {state_class}) -> {state_class}:
    """Retrieve relevant documents from Vector Search."""
    user_query = state["messages"][-1].content

    print(f"Retrieving documents for: {{user_query}}")

    # Retrieve documents
    documents = retriever.get_relevant_documents(user_query)

    return {{
        "retrieved_documents": documents
    }}


def generate_node(state: {state_class}) -> {state_class}:
    """Generate answer using retrieved context."""
    user_query = state["messages"][-1].content
    documents = state.get("retrieved_documents", [])

    # Format context
    if not documents:
        context = "No relevant documents found."
    else:
        context_parts = []
        for i, doc in enumerate(documents, 1):
            source = doc.metadata.get("source", "Unknown")
            context_parts.append(f"[Document {{i}}] (source: {{source}})\\n{{doc.page_content}}")
        context = "\\n\\n".join(context_parts)

    # Generate answer
    prompt = f"""Use the following context to answer the user's question.
If the context doesn't contain relevant information, say so.

Context:
{{context}}

Question: {{user_query}}

Answer:"""

    response = llm.invoke(prompt)
    answer = response.content

    return {{
        "messages": [AIMessage(content=answer)],
        "final_answer": answer
    }}


def create_agent():
    """Create simple RAG agent."""
    # Build graph
    graph = StateGraph({state_class})

    # Add nodes
    graph.add_node("retrieve", retrieve_node)
    graph.add_node("generate", generate_node)

    # Add edges
    graph.set_entry_point("retrieve")
    graph.add_edge("retrieve", "generate")
    graph.add_edge("generate", END)

    # Compile
    return graph.compile()


if __name__ == "__main__":
    import sys

    if len(sys.argv) < 2:
        print("Usage: python {agent_name}.py <query>")
        sys.exit(1)

    query = sys.argv[1]

    # Enable MLflow tracing
    mlflow.langchain.autolog()

    # Create agent
    agent = create_agent()

    # Run agent
    with mlflow.start_run():
        result = agent.invoke({{
            "messages": [HumanMessage(content=query)],
            "retrieved_documents": [],
            "final_answer": ""
        }})

        print("\\nAnswer:")
        print("=" * 60)
        print(result["final_answer"])
        print("=" * 60)

        # Log metrics
        mlflow.log_param("query", query)
        mlflow.log_metric("num_documents", len(result.get("retrieved_documents", [])))
'''


TOOL_CALLING_RAG_TEMPLATE = '''"""
{agent_name} - Tool-calling RAG agent with Vector Search.

The LLM decides when to use the Vector Search tool.

Generated by create_rag_agent.py
"""

import os
from typing import TypedDict, Annotated, Sequence, Optional
import operator

from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage
from langgraph.graph import StateGraph, END
from langgraph.prebuilt import ToolNode
from databricks_langchain import ChatDatabricks
import mlflow

from vector_search_retriever import create_vector_search_tool


# Agent State
class {state_class}(TypedDict):
    """State for {agent_name}."""
    messages: Annotated[Sequence[BaseMessage], operator.add]


# Create Vector Search tool
vector_search_tool = create_vector_search_tool(
    index_name="{index_name}",
    endpoint_name="{endpoint_name}",
    num_results={num_results},
    tool_name="search_knowledge_base",
    tool_description="Search the knowledge base for relevant information to answer user questions"
)

tools = [vector_search_tool]

# Initialize LLM with tools
llm = ChatDatabricks(
    endpoint=os.getenv("DATABRICKS_LLM_ENDPOINT", "databricks-meta-llama-3-1-70b-instruct"),
    temperature=0.1
)
llm_with_tools = llm.bind_tools(tools)


def agent_node(state: {state_class}) -> {state_class}:
    """Agent decides whether to use tools or respond directly."""
    messages = state["messages"]

    # LLM decides next action
    response = llm_with_tools.invoke(messages)

    return {{"messages": [response]}}


def should_continue(state: {state_class}) -> str:
    """Determine if we should call tools or end."""
    last_message = state["messages"][-1]

    # If LLM called tools, execute them
    if hasattr(last_message, "tool_calls") and last_message.tool_calls:
        return "tools"

    # Otherwise, we're done
    return "end"


def create_agent():
    """Create tool-calling RAG agent."""
    # Build graph
    graph = StateGraph({state_class})

    # Add nodes
    graph.add_node("agent", agent_node)
    graph.add_node("tools", ToolNode(tools))

    # Add edges
    graph.set_entry_point("agent")
    graph.add_conditional_edges(
        "agent",
        should_continue,
        {{
            "tools": "tools",
            "end": END
        }}
    )
    graph.add_edge("tools", "agent")

    # Compile
    return graph.compile()


if __name__ == "__main__":
    import sys

    if len(sys.argv) < 2:
        print("Usage: python {agent_name}.py <query>")
        sys.exit(1)

    query = sys.argv[1]

    # Enable MLflow tracing
    mlflow.langchain.autolog()

    # Create agent
    agent = create_agent()

    # Run agent
    with mlflow.start_run():
        result = agent.invoke({{
            "messages": [HumanMessage(content=query)]
        }})

        # Print final answer
        print("\\nConversation:")
        print("=" * 60)
        for msg in result["messages"]:
            if isinstance(msg, HumanMessage):
                print(f"User: {{msg.content}}")
            elif isinstance(msg, AIMessage):
                if hasattr(msg, "tool_calls") and msg.tool_calls:
                    print(f"Agent: [Calling tools: {{[tc['name'] for tc in msg.tool_calls]}}]")
                else:
                    print(f"Agent: {{msg.content}}")
            elif isinstance(msg, ToolMessage):
                print(f"Tool Result: {{msg.content[:200]}}...")
        print("=" * 60)

        # Log metrics
        mlflow.log_param("query", query)
        mlflow.log_metric("num_messages", len(result["messages"]))
'''


SELF_QUERY_RAG_TEMPLATE = '''"""
{agent_name} - Self-query RAG agent with automatic filter extraction.

The LLM extracts structured filters from natural language queries.

Generated by create_rag_agent.py
"""

import os
from typing import TypedDict, Annotated, Sequence, Optional, Dict, Any
import operator

from langchain_core.messages import BaseMessage, HumanMessage, AIMessage
from langgraph.graph import StateGraph, END
from databricks_langchain import ChatDatabricks
from langchain.chains.query_constructor.base import AttributeInfo
import mlflow

from self_query_retriever import DatabricksSelfQueryRetriever


# Agent State
class {state_class}(TypedDict):
    """State for {agent_name}."""
    messages: Annotated[Sequence[BaseMessage], operator.add]
    retrieved_documents: list
    final_response: str


# Define metadata field information
metadata_field_info = [
    AttributeInfo(
        name="source",
        description="The source document name (e.g., 'user_guide.pdf', 'api_reference.md')",
        type="string"
    ),
    AttributeInfo(
        name="category",
        description="Document category: 'tutorial', 'reference', 'guide', or 'api'",
        type="string"
    ),
    AttributeInfo(
        name="page",
        description="The page number in the document",
        type="integer"
    ),
    AttributeInfo(
        name="date",
        description="Document creation date in YYYY-MM-DD format",
        type="string"
    ),
    AttributeInfo(
        name="language",
        description="Programming language (e.g., 'python', 'sql', 'scala')",
        type="string"
    )
]

# Initialize LLM
llm = ChatDatabricks(
    endpoint=os.getenv("DATABRICKS_LLM_ENDPOINT", "databricks-meta-llama-3-1-70b-instruct"),
    temperature=0
)

# Create self-query retriever
self_query_retriever = DatabricksSelfQueryRetriever.from_databricks(
    index_name="{index_name}",
    endpoint_name="{endpoint_name}",
    document_content_description="Technical documentation, tutorials, API references, and code examples",
    metadata_field_info=metadata_field_info,
    llm=llm,
    num_results={num_results},
    verbose=True
)


def self_query_retrieve_node(state: {state_class}) -> {state_class}:
    """
    Retrieve documents using self-query.

    The LLM automatically extracts:
    - Semantic search query
    - Structured metadata filters

    Example queries:
    - "Show me Python tutorials from the user guide"
    - "Find API references created after 2024-01-01"
    - "Get examples from pages 10-20"
    """
    query = state["messages"][-1].content

    print(f"Processing query: {{query}}")

    # Self-query retriever handles:
    # 1. Query understanding
    # 2. Filter extraction
    # 3. Filter translation to Databricks format
    # 4. Filtered similarity search
    docs = self_query_retriever.get_relevant_documents(query)

    print(f"Retrieved {{len(docs)}} documents")

    return {{"retrieved_documents": docs}}


def generate_with_sources_node(state: {state_class}) -> {state_class}:
    """Generate answer with source attribution."""
    query = state["messages"][-1].content
    docs = state["retrieved_documents"]

    if not docs:
        return {{
            "messages": [AIMessage(content="No relevant documents found.")],
            "final_response": "No results"
        }}

    # Format context with rich metadata
    context_parts = []
    for i, doc in enumerate(docs, 1):
        source = doc.metadata.get("source", "Unknown")
        page = doc.metadata.get("page", "N/A")
        category = doc.metadata.get("category", "N/A")
        language = doc.metadata.get("language", "N/A")

        context_parts.append(
            f"[Document {{i}}]\\n"
            f"Source: {{source}}\\n"
            f"Category: {{category}}\\n"
            f"Page: {{page}}\\n"
            f"Language: {{language}}\\n"
            f"Content: {{doc.page_content}}"
        )

    context = "\\n\\n".join(context_parts)

    # Generate with citations
    prompt = f"""Answer the question using the provided documents.
Include citations to sources (e.g., [Document 1], [user_guide.pdf, page 5]).

Context:
{{context}}

Question: {{query}}

Answer with citations:"""

    response = llm.invoke(prompt)

    return {{
        "messages": [AIMessage(content=response.content)],
        "final_response": response.content
    }}


def create_agent():
    """Create self-query RAG agent."""
    # Build graph
    graph = StateGraph({state_class})

    # Add nodes
    graph.add_node("self_query_retrieve", self_query_retrieve_node)
    graph.add_node("generate_with_sources", generate_with_sources_node)

    # Add edges
    graph.set_entry_point("self_query_retrieve")
    graph.add_edge("self_query_retrieve", "generate_with_sources")
    graph.add_edge("generate_with_sources", END)

    # Compile
    return graph.compile()


if __name__ == "__main__":
    import sys

    if len(sys.argv) < 2:
        print("Usage: python {agent_name}.py <query>")
        print("\\nExample queries:")
        print('  python {agent_name}.py "Show me Python tutorials from the user guide"')
        print('  python {agent_name}.py "Find API docs created after 2024-01-01"')
        print('  python {agent_name}.py "Get SQL examples from page 10-20"')
        sys.exit(1)

    query = sys.argv[1]

    # Enable MLflow tracing
    mlflow.langchain.autolog()

    # Create agent
    agent = create_agent()

    # Run agent
    with mlflow.start_run():
        result = agent.invoke({{
            "messages": [HumanMessage(content=query)],
            "retrieved_documents": [],
            "final_response": ""
        }})

        print("\\nAnswer:")
        print("=" * 60)
        print(result["final_response"])
        print("=" * 60)

        # Log metrics
        mlflow.log_param("query", query)
        mlflow.log_metric("num_documents", len(result.get("retrieved_documents", [])))
'''


MULTI_HOP_RAG_TEMPLATE = '''"""
{agent_name} - Multi-hop RAG agent with query decomposition.

Breaks complex queries into sub-questions and retrieves for each.

Generated by create_rag_agent.py
"""

import os
from typing import TypedDict, Annotated, Sequence, List
import operator

from langchain_core.messages import BaseMessage, HumanMessage, AIMessage
from langgraph.graph import StateGraph, END
from databricks_langchain import ChatDatabricks
import mlflow

from vector_search_retriever import create_retriever


# Agent State
class {state_class}(TypedDict):
    """State for {agent_name}."""
    messages: Annotated[Sequence[BaseMessage], operator.add]
    sub_questions: List[str]
    retrieved_documents: List[list]
    final_answer: str


# Initialize components
llm = ChatDatabricks(
    endpoint=os.getenv("DATABRICKS_LLM_ENDPOINT", "databricks-meta-llama-3-1-70b-instruct"),
    temperature=0.1
)

retriever = create_retriever(
    index_name="{index_name}",
    endpoint_name="{endpoint_name}",
    num_results={num_results}
)


def decompose_node(state: {state_class}) -> {state_class}:
    """Decompose complex query into sub-questions."""
    user_query = state["messages"][-1].content

    decompose_prompt = f"""Break down this complex question into 2-4 simpler sub-questions.
Each sub-question should retrieve specific information needed to answer the main question.

Main Question: {{user_query}}

Sub-questions (one per line):"""

    response = llm.invoke(decompose_prompt)

    # Parse sub-questions
    sub_questions = [
        q.strip().lstrip("0123456789.-) ")
        for q in response.content.strip().split("\\n")
        if q.strip()
    ]

    print(f"Decomposed into {{len(sub_questions)}} sub-questions:")
    for i, q in enumerate(sub_questions, 1):
        print(f"  {{i}}. {{q}}")

    return {{"sub_questions": sub_questions}}


def retrieve_multi_node(state: {state_class}) -> {state_class}:
    """Retrieve documents for each sub-question."""
    sub_questions = state.get("sub_questions", [])

    all_documents = []
    for i, sub_q in enumerate(sub_questions, 1):
        print(f"Retrieving for sub-question {{i}}: {{sub_q}}")
        docs = retriever.get_relevant_documents(sub_q)
        all_documents.append(docs)

    return {{"retrieved_documents": all_documents}}


def synthesize_node(state: {state_class}) -> {state_class}:
    """Synthesize final answer from all retrieved context."""
    user_query = state["messages"][-1].content
    sub_questions = state.get("sub_questions", [])
    all_documents = state.get("retrieved_documents", [])

    # Format context from all retrievals
    context_parts = []
    for i, (sub_q, docs) in enumerate(zip(sub_questions, all_documents), 1):
        context_parts.append(f"**Sub-question {{i}}**: {{sub_q}}")
        if docs:
            for j, doc in enumerate(docs, 1):
                source = doc.metadata.get("source", "Unknown")
                context_parts.append(f"  [{{i}}.{{j}}] ({{source}}): {{doc.page_content}}")
        else:
            context_parts.append("  No relevant documents found.")
        context_parts.append("")

    context = "\\n".join(context_parts)

    # Generate final answer
    synthesis_prompt = f"""Use the information retrieved from multiple sub-questions to answer the main question.

Main Question: {{user_query}}

Retrieved Information:
{{context}}

Comprehensive Answer:"""

    response = llm.invoke(synthesis_prompt)
    answer = response.content

    return {{
        "messages": [AIMessage(content=answer)],
        "final_answer": answer
    }}


def create_agent():
    """Create multi-hop RAG agent."""
    # Build graph
    graph = StateGraph({state_class})

    # Add nodes
    graph.add_node("decompose", decompose_node)
    graph.add_node("retrieve_multi", retrieve_multi_node)
    graph.add_node("synthesize", synthesize_node)

    # Add edges
    graph.set_entry_point("decompose")
    graph.add_edge("decompose", "retrieve_multi")
    graph.add_edge("retrieve_multi", "synthesize")
    graph.add_edge("synthesize", END)

    # Compile
    return graph.compile()


if __name__ == "__main__":
    import sys

    if len(sys.argv) < 2:
        print("Usage: python {agent_name}.py <query>")
        sys.exit(1)

    query = sys.argv[1]

    # Enable MLflow tracing
    mlflow.langchain.autolog()

    # Create agent
    agent = create_agent()

    # Run agent
    with mlflow.start_run():
        result = agent.invoke({{
            "messages": [HumanMessage(content=query)],
            "sub_questions": [],
            "retrieved_documents": [],
            "final_answer": ""
        }})

        print("\\nFinal Answer:")
        print("=" * 60)
        print(result["final_answer"])
        print("=" * 60)

        # Log metrics
        mlflow.log_param("query", query)
        mlflow.log_metric("num_sub_questions", len(result.get("sub_questions", [])))
'''


def generate_agent(
    agent_name: str,
    agent_type: str,
    output_dir: Path,
    index_name: str,
    endpoint_name: str,
    num_results: int = 5
):
    """Generate RAG agent implementation."""

    # Prepare template variables
    state_class = f"{agent_name.replace('_', ' ').title().replace(' ', '')}State"

    template_map = {
        "simple": SIMPLE_RAG_TEMPLATE,
        "tool-calling": TOOL_CALLING_RAG_TEMPLATE,
        "multi-hop": MULTI_HOP_RAG_TEMPLATE,
        "self-query": SELF_QUERY_RAG_TEMPLATE
    }

    if agent_type not in template_map:
        raise ValueError(f"Unknown agent type: {agent_type}")

    template = template_map[agent_type]

    content = template.format(
        agent_name=agent_name,
        state_class=state_class,
        index_name=index_name,
        endpoint_name=endpoint_name,
        num_results=num_results
    )

    # Write file
    output_file = output_dir / f"{agent_name}.py"
    output_file.write_text(content)

    print(f"✓ Generated: {output_file}")

    return output_file


def main():
    """Main entry point."""
    parser = argparse.ArgumentParser(
        description="Generate LangGraph RAG agent with Databricks Vector Search"
    )
    parser.add_argument("agent_name", help="Name of the agent")
    parser.add_argument(
        "--type",
        choices=["simple", "tool-calling", "multi-hop", "self-query"],
        default="simple",
        help="Agent type: simple, tool-calling, multi-hop, or self-query RAG"
    )
    parser.add_argument(
        "--index-name",
        required=True,
        help="Vector Search index name (catalog.schema.index)"
    )
    parser.add_argument(
        "--endpoint-name",
        required=True,
        help="Vector Search endpoint name"
    )
    parser.add_argument(
        "--num-results",
        type=int,
        default=5,
        help="Number of results to retrieve (default: 5)"
    )
    parser.add_argument(
        "-o", "--output",
        default=".",
        help="Output directory"
    )

    args = parser.parse_args()

    output_dir = Path(args.output)
    output_dir.mkdir(parents=True, exist_ok=True)

    # Generate agent
    agent_file = generate_agent(
        agent_name=args.agent_name,
        agent_type=args.type,
        output_dir=output_dir,
        index_name=args.index_name,
        endpoint_name=args.endpoint_name,
        num_results=args.num_results
    )

    print(f"\n✓ RAG agent generated successfully!")
    print(f"\nAgent Type: {args.type}")
    print(f"\nNext steps:")
    print(f"1. Review and customize: {agent_file}")
    print(f"2. Set environment variables:")
    print(f"   export DATABRICKS_HOST=https://your-workspace.cloud.databricks.com")
    print(f"   export DATABRICKS_TOKEN=dapi...")
    print(f"   export DATABRICKS_LLM_ENDPOINT=databricks-meta-llama-3-1-70b-instruct")
    print(f"3. Run the agent:")
    print(f'   python {agent_file} "Your question here"')


if __name__ == "__main__":
    main()
